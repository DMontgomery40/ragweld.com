# ragweld

> Open-source RAG workbench with tribrid retrieval (vector + sparse + graph), trainable LoRA reranking, embedded Grafana observability, and a visual configuration wizard. MIT licensed.

ragweld makes RAG pipelines visible and controllable. Instead of black-box APIs or framework code, you get a GUI where you can see every retrieval stage, tune 396 parameters through a wizard, train a domain-specific reranker from user feedback, and monitor everything through embedded Grafana dashboards.

## Core Concepts

- [Tribrid Retrieval](https://ragweld.com/glossary/): Combines dense vector search (Qdrant), sparse lexical search (BM25/Redis), and knowledge graph traversal (Neo4j) into a single pipeline with configurable fusion weights.
- [Trainable LoRA Reranker](https://ragweld.com/blog/posts/learning-reranker-qwen3-mlx/): Fine-tunes a lightweight reranker on your domain data using thumbs-up/down feedback. Runs on Apple Silicon via MLX or GPU via PyTorch.
- [Evaluation Framework](https://ragweld.com/glossary/eval-dataset-path/): Built-in eval with golden datasets, run diffs, regression detection, and drilldowns per query.
- [Embedded Observability](https://ragweld.com/glossary/grafana-url/): Grafana dashboards embedded in split-view alongside the chat interface. See latency, token usage, retrieval scores in real time.
- [Chat Memory with RAG Hybrid Mode](https://ragweld.com/blog/posts/when-to-query-chat-memory-vs-your-corpus/): Smart gating decides when to query conversation history, the corpus, or both.

## Documentation

- [Live Demo](https://ragweld.com/demo/): Interactive demo of the full ragweld interface.
- [Parameter Glossary](https://ragweld.com/glossary/): Searchable reference for all 396 RAG configuration parameters with descriptions, categories, and links.
- [Full Documentation](https://dmontgomery40.github.io/ragweld/latest/configuration/): Technical docs hosted on GitHub Pages.
- [GitHub Repository](https://github.com/DMontgomery40/ragweld): Source code (MIT license).

## Blog

- [Qwen3 LoRA Learning Reranker on Apple Silicon](https://ragweld.com/blog/posts/learning-reranker-qwen3-mlx/): How ragweld trains and serves a Qwen3 yes/no-logit reranker with MLX on Apple Silicon.
- [When to Query Chat Memory vs. Your Corpus](https://ragweld.com/blog/posts/when-to-query-chat-memory-vs-your-corpus/): Practical retrieval policy for deciding when to hit chat memory, when to hit the corpus, and how to avoid token/latency blowups at scale.

## Architecture

ragweld is a Python backend (FastAPI) with a React frontend (Vite). Key infrastructure:

- **Vector store**: Qdrant (dense embeddings via OpenAI, Voyage, Ollama, or custom)
- **Sparse store**: Redis (BM25 tokenization with configurable stop words)
- **Graph store**: Neo4j (entity/relationship extraction, graph traversal for multi-hop queries)
- **Reranker**: LoRA fine-tuned model (Qwen3-0.6B default) trained from user feedback triplets
- **Observability**: Embedded Grafana with pre-built RAG dashboards
- **Config**: 396 parameters managed through Pydantic models, exposed via 5-step setup wizard

## API

The ragweld API is RESTful (FastAPI with OpenAPI docs at `/docs`). Key endpoints:

- `POST /query` — Run a retrieval query with configurable pipeline stages
- `POST /index` — Index documents (supports code repos, PDFs, markdown)
- `POST /eval/run` — Execute evaluation against golden dataset
- `GET /config` — Read current configuration
- `PUT /config` — Update configuration parameters
- `POST /reranker/train` — Trigger reranker fine-tuning from feedback data

## Pricing

- **Self-hosted**: Free forever (MIT license)
- **Hosted Solo**: $149/month (managed infrastructure, priority support)
- **Hosted Team**: $399/month (team features, shared dashboards, SSO)
- **Enterprise**: Custom pricing (on-prem deployment, SLAs, dedicated support)


## Full Parameter Glossary

All 396 configurable parameters in ragweld, grouped by category.

### chunking

- **AST Overlap Lines** (`AST_OVERLAP_LINES`): Number of overlapping lines between consecutive AST-based code chunks. Overlap ensures context continuity across chunk boundaries, preventing loss of meaning when functions or classes are split. Hi...
- **Chunk Overlap** (`CHUNK_OVERLAP`): Number of characters overlapped between adjacent chunks. Overlap reduces boundary effects and improves recall at the cost of a larger index and slower indexing.
- **Chunk Size** (`CHUNK_SIZE`): Target size (in characters) for each indexed chunk. For AST chunking this acts as a guardrail when nodes are large. Larger chunks preserve more context but reduce recall; smaller chunks improve rec...
- **Chunking Strategy** (`CHUNKING_STRATEGY`): Primary strategy for splitting code into chunks during indexing. Options: "ast" (AST-aware, syntax-respecting, recommended for code), "greedy" (line-based splitting, simpler), "hybrid" (AST with gr...
- **Emit chunk ordinal** (`EMIT_CHUNK_ORDINAL`): When enabled, chunking emits a stable <code>chunk_ordinal</code> into each chunk’s metadata (0-based). This enables neighbor-window retrieval, which pulls adjacent chunks around top hits for better...
- **Emit parent doc id** (`EMIT_PARENT_DOC_ID`): When enabled, chunking emits a stable <code>parent_doc_id</code> into each chunk’s metadata (usually the file path). This helps downstream retrieval and debugging group chunks by document.
- **Enrich Code Chunks** (`ENRICH_CODE_CHUNKS`): Enable per-chunk code summarization during indexing. When on, each code chunk gets an AI-generated summary and keywords stored alongside the code. Powers the Cards feature (high-level code summarie...
- **Hugging Face tokenizer name** (`TOKENIZATION_HF_TOKENIZER_NAME`): Tokenizer name for <code>huggingface</code> tokenization strategy (e.g., <code>gpt2</code> or your embedding model’s tokenizer). Requires the backend to have Transformers installed.
- **Index max file size (MB)** (`INDEX_MAX_FILE_SIZE_MB`): Hard cap for indexing: files larger than this (in MB) are skipped before reading. For very large text dumps, raise this above the file size and enable streaming ingestion.
- **Large file mode** (`LARGE_FILE_MODE`): How to ingest very large text files.<br><br><b>stream</b>: reads in bounded character blocks (avoids loading the full file into memory).<br><b>read_all</b>: reads the entire file into memory before...
- **Large file stream chunk chars** (`LARGE_FILE_STREAM_CHUNK_CHARS`): When <code>LARGE_FILE_MODE=stream</code>, this controls the approximate number of characters to read per block. Larger blocks reduce boundary artifacts but increase memory usage.
- **Markdown include code fences** (`MARKDOWN_INCLUDE_CODE_FENCES`): When using markdown chunking, controls whether fenced code blocks are included in their surrounding section. Disable if code fences add noise for your corpus.
- **Markdown max heading level** (`MARKDOWN_MAX_HEADING_LEVEL`): When using markdown chunking, split on headings up to this level (e.g., 1–4 splits on #, ##, ###, ####). Deeper headings are treated as normal text.
- **Max Chunk Tokens** (`MAX_CHUNK_TOKENS`): Maximum token length for a single code chunk during AST-based chunking. Limits chunk size to fit within embedding model token limits (typically 512-8192 tokens). Larger chunks (1000-2000 tokens) ca...
- **Max tokens per chunk (hard)** (`TOKENIZATION_MAX_TOKENS_PER_CHUNK_HARD`): Hard ceiling on tokens per chunk used as a safety limit for chunking and embedding input budgeting. Prevents runaway chunk sizes.
- **Min Chunk Chars** (`MIN_CHUNK_CHARS`): Minimum character count for a valid chunk. Chunks smaller than this are discarded or merged with adjacent chunks to avoid indexing trivial code fragments (empty functions, single-line comments, imp...
- **Overlap tokens** (`OVERLAP_TOKENS`): Token overlap between adjacent chunks for token-based strategies. Overlap preserves context across boundaries but increases total tokens indexed.
- **Recursive max depth** (`RECURSIVE_MAX_DEPTH`): Maximum recursion depth for separator-based (recursive) chunking. Higher values try more separators before falling back to hard token windows.
- **Separator keep** (`SEPARATOR_KEEP`): When splitting by separators in recursive chunking, controls whether the separator is kept as part of the left chunk (suffix), right chunk (prefix), or dropped (none).
- **Separators (recursive chunking)** (`SEPARATORS`): Separator list used by recursive chunking, in priority order (e.g., <code>\n\n</code>, <code>\n</code>, <code>. </code>, <code> </code>, <code>""</code>). The empty separator is the hard fallback s...
- **Target tokens** (`TARGET_TOKENS`): Target tokens per chunk for token-based chunking strategies. Smaller chunks improve retrieval granularity but increase index size.
- **tiktoken encoding** (`TOKENIZATION_TIKTOKEN_ENCODING`): The tiktoken encoding name used for token counting, chunking, and input budgeting (e.g., <code>o200k_base</code>). If the encoding is unavailable, the backend falls back to a safe default.
- **Tokenization estimate-only** (`TOKENIZATION_ESTIMATE_ONLY`): When enabled, token counts use a fast heuristic estimate instead of running a real tokenizer. Useful for huge corpora when exact token boundaries are not critical.
- **Tokenization lowercase** (`TOKENIZATION_LOWERCASE`): Lowercase text before tokenization and chunking. Keep off for legal dumps, names, or cases where capitalization carries meaning.
- **Tokenization normalize Unicode** (`TOKENIZATION_NORMALIZE_UNICODE`): Normalize Unicode to NFKC before tokenization. This improves consistency across visually similar characters and reduces tokenization surprises in messy corpora.
- **Tokenization strategy** (`TOKENIZATION_STRATEGY`): Tokenizer implementation used for token-aware chunking and embedding input budgeting.<br><br><b>tiktoken</b>: fast + accurate for OpenAI-style tokenization (recommended).<br><b>whitespace</b>: simp...

### embedding

- **Auto-set embedding dimensions** (`EMBEDDING_AUTO_SET_DIMENSIONS`): When enabled, the UI automatically sets <code>embedding_dim</code> to the dimension declared for the selected embedding model in <code>data/models.json</code>. This prevents subtle mismatches that ...
- **Contextual chunk embeddings** (`EMBEDDING_CONTEXTUAL_CHUNK_EMBEDDINGS`): Controls whether chunk embeddings incorporate surrounding context.<br><br><b>off</b>: embed each chunk independently (default).<br><b>prepend_context</b>: embed chunk text with a small amount of co...
- **Editor Embed Mode** (`EDITOR_EMBED_ENABLED`): Enable embedded editor iframe in GUI (1=yes, 0=no). When enabled, editor opens inline. When disabled, opens in new tab/window. Embedding requires CORS configuration.
- **Embedding backend** (`EMBEDDING_BACKEND`): Selects which embedding implementation is used during indexing and retrieval.<br><br><b>deterministic</b>: produces stable fake embeddings for tests/offline development (no API calls). Retrieval wo...
- **Embedding Batch Size** (`EMBEDDING_BATCH_SIZE`): Number of text chunks to embed in a single API call or local batch during indexing. Higher values (50-200) speed up indexing by reducing API round trips but may hit rate limits or memory constraint...
- **Embedding Cache** (`EMBEDDING_CACHE_ENABLED`): Cache embedding API results to disk to avoid re-computing vectors for identical text. Reduces API costs and speeds up reindexing. Disable only for debugging or when embeddings change frequently.
- **Embedding Configuration Valid** (`EMBEDDING_MATCH`): Your current embedding configuration matches what was used to create the index. Search results will be accurate and relevant. The vectors in your index are compatible with queries generated using y...
- **Embedding Dimension** (`EMBEDDING_DIM`): Vector dimensionality for MXBAI/local embedding models. Common sizes: 384 (fast, lower quality), 768 (balanced, recommended), 1024 (best quality, slower). Larger dimensions capture more semantic nu...
- **Embedding input truncation** (`EMBEDDING_INPUT_TRUNCATION`): What to do when a single chunk exceeds the embedding model’s maximum input tokens.<br><br><b>truncate_end</b>: keep the start of the text (recommended for most docs).<br><b>truncate_middle</b>: kee...
- **Embedding Max Retries** (`EMBEDDING_RETRY_MAX`): Retry attempts for failed embedding API calls during indexing. Higher values ensure indexing completes despite transient errors but slow down failure recovery. Typical: 2-3 retries.
- **Embedding Max Tokens** (`EMBEDDING_MAX_TOKENS`): Maximum token length for text chunks sent to embedding models during indexing. Text exceeding this length is truncated by the tokenizer. Most embedding models support 512-8192 tokens. Longer limits...
- **Embedding Model (OpenAI)** (`EMBEDDING_MODEL`): OpenAI embedding model name when EMBEDDING_TYPE=openai. Current options: "text-embedding-3-small" (512-3072 dims, $0.02/1M tokens, fast), "text-embedding-3-large" (256-3072 dims, $0.13/1M tokens, h...
- **Embedding Provider** (`EMBEDDING_TYPE`): Selects the embedding provider for dense vector search. Also determines the token counter used during code chunking, which affects chunk boundaries and splitting behavior. • openai — strong quality...
- **Embedding text prefix** (`EMBEDDING_TEXT_PREFIX`): Optional string prepended to every text before embedding. Use to add stable context (e.g., "document: ") or to enforce consistent formatting across corpora. Changing this requires reindexing.
- **Embedding text suffix** (`EMBEDDING_TEXT_SUFFIX`): Optional string appended to every text before embedding. Useful for adding a delimiter or stable footer context. Changing this requires reindexing.
- **Embedding Timeout** (`EMBEDDING_TIMEOUT`): Maximum seconds to wait for embedding API response. Similar to GEN_TIMEOUT but for embedding calls during indexing. Increase for large batches or slow networks. Typical: 30-60 seconds.
- **Embedding Type Mismatch** (`EMBEDDING_MISMATCH`): Your current embedding configuration differs from what was used to create your index. This is a CRITICAL issue that will cause search to return completely irrelevant results. Embeddings are mathema...
- **Indexing Batch Size** (`INDEXING_BATCH_SIZE`): Number of chunks to process in parallel during the indexing pipeline (chunking, enrichment, embedding, Qdrant upload). Higher values (100-500) maximize throughput on fast networks and powerful mach...
- **Late chunking max doc tokens** (`EMBEDDING_LATE_CHUNKING_MAX_DOC_TOKENS`): Upper bound on document tokens processed for late chunking. Late chunking embeds a whole document (or long span) once, so this protects memory/latency. Only used when contextual embeddings are set ...
- **Local Embedding Model** (`EMBEDDING_MODEL_LOCAL`): HuggingFace model name or local path when EMBEDDING_TYPE=local or mxbai. Popular options: "mixedbread-ai/mxbai-embed-large-v1" (1024 dims, excellent quality), "BAAI/bge-small-en-v1.5" (384 dims, fa...
- **Reranker Batch Size (Inference)** (`TriBridRAG_RERANKER_BATCH`): Batch size used when scoring candidates during rerank. Higher values reduce latency but increase memory. If you see OOM or throttling, lower this value.
- **Training Batch Size** (`RERANKER_TRAIN_BATCH`): Batches per gradient step during training. Larger batch sizes stabilize training but require more memory. For Colima or small GPUs/CPUs, use 1–4. If you see the container exit with code -9 (OOM), r...
- **Voyage Embed Dim** (`VOYAGE_EMBED_DIM`): Embedding vector dimension when using Voyage embeddings (provider‑specific). Larger dims can improve recall but increase Qdrant storage. Must match the output dimension of your chosen Voyage model ...

### evaluation

- **Compare With (BEFORE)** (`EVAL_COMPARE_RUN`): Optionally select a previous evaluation run to compare against. This enables the configuration diff view showing exactly what parameters changed between runs, and highlights regressions (questions ...
- **Eval Analysis** (`EVAL_ANALYSIS_SUBTAB`): View and compare RAG evaluation runs. Analyze retrieval accuracy metrics, see question-by-question results, compare configuration changes between runs, and get AI-powered insights on performance re...
- **Eval Final‑K** (`EVAL_FINAL_K`): Number of top results to consider when evaluating Hit@K metrics. If set to 10, eval checks if the expected answer appears in the top 10 results. Lower values (5) test precision, higher values (20) ...
- **Eval Multi‑Query** (`EVAL_MULTI`): Enable multi-query expansion during evaluation runs (1=yes, 0=no). When enabled, each golden question is rewritten multiple times (per MQ_REWRITES setting) to test recall under query variation. Tur...
- **Evaluation Logs Terminal** (`EVAL_LOGS_TERMINAL`): Open the sliding terminal to stream raw evaluation output (question-by-question) and verify the exact settings used for the last run.
- **Layer Bonus (Retrieval)** (`LAYER_BONUS_RETRIEVAL`): Score boost applied to chunks from backend/API/service layers when query intent is classified as retrieval or data-related. Complements LAYER_BONUS_GUI for multi-tier architecture routing. When use...
- **Metrics Enabled** (`METRICS_ENABLED`): Enable Prometheus metrics collection and /metrics endpoint. When on, exposes query latency, cache hits, error rates, etc. Essential for production monitoring. Minimal overhead.
- **Primary K Override (@k cutoff)** (`RERANKER_TRAIN_PRIMARY_K_OVERRIDE`): <span class="tt-strong">What “k” means</span><br>For metrics like <span class="mono">MRR@k</span> and <span class="mono">nDCG@k</span>, <span class="mono">k</span> is the <span class="tt-strong">cu...
- **Primary Metric Override** (`RERANKER_TRAIN_PRIMARY_METRIC_OVERRIDE`): <span class="tt-strong">What this setting does</span><br>Overrides the run’s “North Star” metric <span class="tt-strong">only for the next run you start</span>. If you leave it on <span class="mono...
- **Primary Run (AFTER)** (`EVAL_PRIMARY_RUN`): Select the evaluation run to analyze. This is typically the most recent run you want to inspect. When comparing, this is the "AFTER" run showing your latest configuration changes. The accuracy metr...
- **Recommended Metric (North Star)** (`RERANKER_TRAIN_RECOMMENDED_METRIC`): <span class="tt-strong">What this is</span><br>This is the <span class="tt-strong">one headline number</span> the Training Studio treats as the “North Star” for your corpus. It is chosen automatica...
- **Run RAG Evaluation** (`RUN_EVAL_ANALYSIS`): Execute the full RAG evaluation suite using your current configuration settings. This runs all golden questions through the retrieval pipeline and measures Top-1 and Top-K accuracy. A live terminal...
- **Sample Size (Quick vs Full)** (`EVAL_SAMPLE_SIZE`): Limit evaluation to a subset of golden questions for faster iteration and testing. Quick (10): ~1 minute, good for sanity checks. Medium (25): ~2-3 minutes, better coverage. Large (50): ~5 minutes,...

### general

- **Active Repository** (`REPO`): Logical repository name for routing and indexing. MCP and CLI use this to scope retrieval. Must match a repository name defined in repos.json for multi-repo setups. Example: "tribrid-demo", "myapp"...
- **Advanced Parameters** (`ADVANCED_RAG_TUNING`): Expert controls for fusion weighting, score bonuses, and iteration behavior. These significantly affect retrieval quality and performance. Change only if you understand trade-offs.
- **Alert Webhook Timeout** (`ALERT_WEBHOOK_TIMEOUT`): Maximum seconds to wait for alert webhook response (Slack, Discord, etc.). Prevents slow webhooks from blocking the main process. Typical: 5-10 seconds.
- **Answer Confidence Threshold** (`CHAT_CONFIDENCE_THRESHOLD`): Minimum retrieval confidence to return an answer without fallback. Lower values return more answers (risking guesses); higher values are conservative.
- **Auto-Generate Keywords** (`KEYWORDS_AUTO_GENERATE`): Automatically extract repository keywords from code and documentation during indexing (1=yes, 0=no). When enabled, the system analyzes class names, function names, docstrings, and comments to build...
- **Auto-index conversations** (`chat.recall.auto_index`): <span class="tt-strong">Recall</span> is TriBridRAG’s persistent chat memory. It stores your conversation as a special internal corpus (typically <span class="mono">recall_default</span>) so it can...
- **Auto-Open Browser** (`OPEN_BROWSER`): Automatically open browser to GUI when server starts (1=yes, 0=no). Convenient for local development, disable for server deployments or headless environments.
- **Auto-open LangSmith** (`TRACE_AUTO_LS`): UI convenience flag intended to auto-open LangSmith after a request (1=yes, 0=no). TriBridRAG does not currently implement LangSmith deep-linking; this setting is reserved for future integration.
- **Auto-Start Colima** (`AUTO_COLIMA`): Automatically start Colima Docker runtime if not running (macOS only, 1=yes, 0=no). Convenient for local development, ensures Docker containers start without manual intervention.
- **Auto‑Scroll to New Messages** (`CHAT_AUTO_SCROLL`): Automatically scrolls the conversation to the newest message. Disable when reviewing earlier context while messages stream.
- **Baseline Path** (`BASELINE_PATH`): Directory where evaluation loop saves baseline results for regression testing and A/B comparison. Each eval run's metrics (Hit@K, MRR, latency) are stored here with timestamps. Use this to ensure r...
- **Card Semantic Bonus** (`CARD_BONUS`): Score bonus when a result matches code "Cards" (semantic summaries from enrichment). Improves intent‑based retrieval (e.g., "where is auth handled?"). Requires ENRICH_CODE_CHUNKS.
- **Cards Max** (`CARDS_MAX`): Maximum number of summary cards to load and consider during retrieval for score boosting. Cards are high-level summaries of code modules/features. Lower values (10-20) are faster but may miss relev...
- **Chat Configuration** (`CHAT_SETTINGS`): Settings that control model, answer length, rewrite strategy, and retrieval size for the chat interface. These affect latency, cost, and answer quality.
- **Chat History Storage** (`CHAT_HISTORY`): Controls how chat history is saved and loaded. History persists in browser localStorage only — no server storage for privacy.
- **Chat Streaming** (`CHAT_STREAMING_ENABLED`): Enable streaming responses for chat interfaces. When on, tokens appear incrementally (like typing). Better UX but requires SSE support. Disable for simple request-response APIs.
- **Chunk Entity Expansion Enabled** (`GRAPH_CHUNK_ENTITY_EXPANSION_ENABLED`): When graph mode is "chunk", expand from seed chunks via entity graph (IN_CHUNK links) to find related chunks. This combines chunk-based retrieval with entity relationships for better recall. When e...
- **Chunk Entity Expansion Weight** (`GRAPH_CHUNK_ENTITY_EXPANSION_WEIGHT`): Blend weight for entity-expansion scores relative to seed chunk scores when entity expansion is enabled. Higher values (0.7-1.0) favor entity-expanded chunks, lower values (0.3-0.6) favor seed chun...
- **Chunk Neighbor Window** (`GRAPH_CHUNK_NEIGHBOR_WINDOW`): When graph mode is "chunk", include up to N adjacent chunks (NEXT_CHUNK relationships) around each seed hit. Higher values (2-5) include more context but may introduce noise. Lower values (0-1) are...
- **Chunk Seed Overfetch Multiplier** (`GRAPH_CHUNK_SEED_OVERFETCH`): When graph mode is "chunk" and Neo4j uses a shared database, overfetch seed hits before filtering by corpus_id. This compensates for shared database queries where corpus filtering happens after ret...
- **Chunk Summaries Enrich Default** (`CHUNK_SUMMARIES_ENRICH_DEFAULT`): Enable chunk summary enrichment by default when building summaries. When enabled, summaries include enriched metadata (detailed purpose, technical details, domain concepts) using LLM analysis. When...
- **Clear Python bytecode caches** (`DEV_STACK_CLEAR_PYTHON_BYTECODE`): Clears <span class="tt-strong">Python bytecode caches</span> inside this repo and then triggers a backend reload.<br><br><span class="tt-strong">What it deletes</span> (repo-owned only):<br>- <span...
- **Code Block Highlighting** (`CHAT_SYNTAX_HIGHLIGHT`): Apply syntax highlighting to code blocks in responses. Improves readability in multi‑language projects. May increase render time on very long messages.
- **Code Cards** (`CODE_CARDS`): High‑level semantic summaries of code chunks, built during enrichment. Cards enable intent‑based retrieval and better filtering for conceptual queries.
- **Code Indexing** (`INDEXING`): Indexing turns a corpus folder into the data structures TriBridRAG can search. It’s corpus-scoped: each corpus has its own storage, graph, and configuration. During indexing, TriBridRAG typically p...
- **Cohere API Key** (`COHERE_API_KEY`): API key for Cohere reranking when RERANK_BACKEND=cohere.
- **Colima Profile** (`COLIMA_PROFILE`): Colima profile name to use when AUTO_COLIMA is enabled. Profiles allow different Docker VM configurations (CPU, memory, disk). Default profile used if empty.
- **Collection Name** (`COLLECTION_NAME`): Optional override for the Qdrant collection name where vectors are stored. Defaults to code_chunks_{REPO}. Set this if you maintain multiple profiles, A/B test embedding models, or run parallel ind...
- **Collection Suffix** (`COLLECTION_SUFFIX`): Optional string appended to the default collection name (code_chunks_{REPO}) for A/B testing different indexing strategies. For example, suffix "_v2" creates "code_chunks_myrepo_v2". Useful when co...
- **Confidence Any** (`CONF_ANY`): Fallback threshold - proceed with retrieval if ANY single result exceeds this score, even if top-1 or avg-5 thresholds aren't met. This prevents the system from giving up when there's at least one ...
- **Confidence Avg-5** (`CONF_AVG5`): Average confidence score of the top-5 results, used as a gate for query rewriting iterations. If avg(top-5) is below this threshold, the system may rewrite the query and try again. Lower values (0....
- **Confidence Top-1** (`CONF_TOP1`): Minimum confidence score (0.0-1.0) required to accept the top-1 result without further processing. If the best result scores above this threshold, it's returned immediately. Lower values (0.55-0.60...
- **Containers (running/total)** (`SYS_STATUS_CONTAINERS`): Shows Docker container health for <span class="tt-strong">this TriBridRAG stack</span> as <span class="mono">running/total</span>.<br><br><span class="tt-strong">running</span>: containers whose st...
- **Corpora (active selection)** (`SYS_STATUS_CORPUS`): A <span class="tt-strong">corpus</span> is TriBridRAG’s unit of isolation: each corpus has its own indexing storage (Postgres), graph storage (Neo4j), and per-corpus configuration.<br><br>This Syst...
- **Custom System Prompt** (`CHAT_SYSTEM_PROMPT`): Override the default expert system prompt for Chat. Use to adjust tone, safety constraints, or provide domain instructions. Leave empty to use the built‑in TriBridRAG expert prompt.
- **Data Directory** (`DATA_DIR`): Base directory for application data storage (logs, tracking, temp files). Defaults to ./data. Change if you need data stored elsewhere or shared across deployments.
- **Dev Local Uvicorn** (`DEV_LOCAL_UVICORN`): Run Uvicorn ASGI server in direct Python mode instead of Docker for faster development iteration (1=yes, 0=no). Enables hot-reload and easier debugging. Production should use 0 (Docker).
- **Disable Enrichment** (`ENRICH_DISABLED`): Completely disable code enrichment (summaries, keywords, cards) during indexing (1=disable, 0=enable). When disabled, indexing is much faster and cheaper (no LLM API calls) but you lose card search...
- **Documentation Directory** (`DOCS_DIR`): Path to the documentation directory containing markdown files, API references, and user guides. This directory is served at /docs/* by the FastAPI static file handler, making documentation accessib...
- **Edition** (`TRIBRID_EDITION`): Product edition identifier for feature gating in multi-tier deployments. Values: "oss" (open source, all community features), "pro" (professional tier with advanced features), "enterprise" (full fe...
- **Edition** (`TriBridRAG_EDITION`): Product edition identifier for feature gating in multi-tier deployments. Values: "oss" (open source, all community features), "pro" (professional tier with advanced features), "enterprise" (full fe...
- **Endpoint Call Frequency (calls/min)** (`ENDPOINT_CALL_FREQUENCY`): Alert when a single API endpoint receives this many calls per minute. Detects infinite loops, polling gone wrong, or DDoS-like patterns. For example, if /api/search is called 100 times/min for 2+ m...
- **Enrichment Backend** (`ENRICH_BACKEND`): Backend service for generating code summaries and enrichment metadata during indexing. Options: "openai" (GPT models, highest quality), "ollama" (local models, free), "mlx" (Apple Silicon optimized...
- **Error Rate Threshold (%)** (`ERROR_RATE_THRESHOLD`): Percentage threshold for triggering error rate alerts. When the error rate across all requests exceeds this percentage over a 5-minute window, Grafana will trigger an alert. Typical values: 5% for ...
- **Exclude Directories** (`CHUNK_SUMMARIES_EXCLUDE_DIRS`): List of directory paths to skip when building chunk summaries. Directories matching these paths (exact or prefix) are excluded from summarization. Useful for excluding test files, documentation, ge...
- **Exclude Directories** (`EXCLUDE_PATHS`): Comma‑separated directories to exclude when building semantic Code Cards or indexing. Examples: node_modules, vendor, dist.
- **Exclude Keywords** (`CHUNK_SUMMARIES_EXCLUDE_KEYWORDS`): List of keywords that, when present in code, cause the chunk to be skipped during summarization. Useful for excluding deprecated code, TODO comments, legacy implementations, or experimental feature...
- **Exclude Patterns** (`CHUNK_SUMMARIES_EXCLUDE_PATTERNS`): List of glob patterns (e.g., "*.min.js", "*.lock", "**/*.test.ts") to skip when building chunk summaries. Files matching these patterns are excluded from summarization. Useful for excluding generat...
- **Fallback Confidence Threshold** (`CONF_FALLBACK`): When initial retrieval confidence falls below this threshold, triggers a fallback with expanded query rewrites. Lower = more aggressive fallback. Typical: 0.5–0.7.
- **Filename Exact Match Multiplier** (`FILENAME_BOOST_EXACT`): Score multiplier applied when the filename matches the query exactly (e.g., auth.py). Increase to prioritize file‑specific queries.
- **Files Root Override** (`FILES_ROOT`): Override the root directory for the /files HTTP mount point. This setting controls where the FastAPI static file server looks for files when serving requests to /files/*. By default, TriBridRAG use...
- **Final Top‑K** (`FINAL_K`): Number of top results to return after hybrid fusion, reranking, and scoring boosts. This is what you get back from search. Higher values (15-30) provide more context but may include noise. Lower va...
- **Frequency Penalty** (`FREQUENCY_PENALTY`): Penalizes tokens that appear frequently in the generated text so far. Higher values reduce repetition and boilerplate; lower values allow more reuse of prior tokens. Typical ranges: 0.0-0.5 for cod...
- **Freshness Bonus** (`FRESHNESS_BONUS`): Score boost applied to recently modified files during reranking, prioritizing newer code over stale code. Based on file modification time (mtime). Files modified in the last N days receive the full...
- **Fusion Method** (`FUSION_METHOD`): Method for combining results from vector, sparse, and graph search: "rrf" (Reciprocal Rank Fusion) or "weighted" (score-based weighted sum). RRF combines ranking positions without score normalizati...
- **Golden Questions Path** (`GOLDEN_PATH`): Filesystem path to your golden questions JSON file (default: golden.json). Golden questions are curated query-answer pairs used to evaluate retrieval quality through automated testing. Format: [{"q...
- **Google API Key** (`GOOGLE_API_KEY`): API key for Google Gemini models and embedding endpoints (gemini-1.5-pro, gemini-1.5-flash, text-embedding-004). Required when using Google AI services. Create key at Google AI Studio. Gemini 1.5 P...
- **Grafana Auth Mode** (`GRAFANA_AUTH_MODE`): Authentication method for Grafana. Options: "token" (API token), "basic" (username/password), "none" (public dashboards).
- **Grafana Auth Token** (`GRAFANA_AUTH_TOKEN`): API token or service account token for Grafana authentication. Generate in Grafana under Configuration > API Keys or Service Accounts.
- **Graph Max Hops** (`GRAPH_MAX_HOPS`): Maximum number of graph traversal hops from seed nodes. Each hop expands the search to connected nodes (chunks, entities, relationships). Higher values (3-5) find more distant relationships but inc...
- **Graph Search Enabled** (`GRAPH_SEARCH_ENABLED`): Enable or disable graph-based search using Neo4j. When enabled, queries traverse the knowledge graph to find related chunks through entity relationships, code structure (AST), and community detecti...
- **Graph Search Mode** (`GRAPH_SEARCH_MODE`): Graph retrieval strategy: "chunk" uses lexical chunk nodes with Neo4j vector index for semantic chunk search, "entity" uses the legacy code-entity graph. Chunk mode (recommended) combines vector si...
- **Graph Search Top-K** (`GRAPH_SEARCH_TOP_K`): Number of candidate results to retrieve from Neo4j graph search before fusion. Higher values (40-100) improve recall for graph-based relationships but increase query latency. Lower values (15-30) a...
- **Graph Weight** (`FUSION_GRAPH_WEIGHT`): Weight assigned to graph (Neo4j) search results in weighted fusion mode. Higher values (0.4-0.6) favor structural relationships, lower values (0.2-0.3) reduce graph influence. Weights must sum to ~...
- **Greedy Fallback Target (Chars)** (`GREEDY_FALLBACK_TARGET`): Target chunk size (in characters) for greedy fallback chunking when AST-based chunking fails or encounters oversized logical units. Greedy chunking splits text at line boundaries to hit this approx...
- **History Limit** (`CHAT_HISTORY_LIMIT`): Maximum number of messages to retain in local history. Older messages are pruned when the limit is reached. Typical range: 50–1000.
- **Hydration Max Chars** (`HYDRATION_MAX_CHARS`): Maximum characters to load per chunk when hydrating results with code content. Prevents huge chunks from bloating responses and consuming excessive memory. 0 = no limit (may cause memory issues wit...
- **Hydration Mode** (`HYDRATION_MODE`): Controls when full code is loaded from chunks.jsonl. "Lazy" (recommended) loads code after retrieval, providing full context with minimal memory overhead. "None" returns only metadata (file path, l...
- **Include Communities** (`GRAPH_INCLUDE_COMMUNITIES`): Enable community-based expansion in graph search. When enabled, the system uses community detection algorithms (e.g., Louvain) to identify clusters of related nodes and expands search to include en...
- **Include Thinking in Stream** (`CHAT_STREAM_INCLUDE_THINKING`): When enabled and using a thinking/reasoning model (like Anthropic Claude with extended thinking or OpenAI o-series), the model's reasoning process will be streamed to the UI before the final answer...
- **Index delay (seconds)** (`chat.recall.index_delay_seconds`): How long TriBridRAG waits before running the background Recall indexing job after a response completes.<br><br>A shorter delay makes it more likely that Recall can immediately “remember” the last e...
- **Inline File References** (`CHAT_SHOW_CITATIONS`): Display source file paths and line numbers inline with the answer. Citations become clickable links to code locations.
- **Intent Matrix (Advanced)** (`LAYER_INTENT_MATRIX`): Advanced JSON map that biases results toward specific architectural layers based on the detected intent of the query. Structure: { intent: { layer: multiplier } } When a query is classified as an i...
- **Keywords Boost** (`KEYWORDS_BOOST`): Score boost multiplier applied to search results that match corpus keywords. Higher values (1.5-2.0) strongly favor keyword matches, lower values (1.1-1.3) provide mild preference. The boost is mul...
- **Keywords Max Per Repo** (`KEYWORDS_MAX_PER_REPO`): Maximum number of repository-specific keywords to extract and store for query routing in multi-repo setups. Higher values (100-200) capture more routing signals but increase memory and may introduc...
- **Keywords Min Frequency** (`KEYWORDS_MIN_FREQ`): Minimum term frequency required for a keyword to be included. Terms must appear at least this many times in the corpus to be considered. Higher values (5-10) ensure keywords are common enough to be...
- **Keywords Refresh (Hours)** (`KEYWORDS_REFRESH_HOURS`): How often (in hours) to regenerate repository keywords from code for improved query routing. Lower values keep keywords fresh but increase indexing overhead. Typical: 24-168 hours (1-7 days).
- **LangChain API Key** (`LANGCHAIN_API_KEY`): Alternate env var name used by LangSmith. Treat as an alias for LANGSMITH_API_KEY (external provider). TriBridRAG does not currently export traces to LangSmith.
- **LangChain Endpoint** (`LANGCHAIN_ENDPOINT`): LangSmith API endpoint URL (external provider). Stored in config field tracing.langchain_endpoint. Reserved for future integration.
- **LangChain Legacy** (`LANGCHAIN_LEGACY`): DEPRECATED: Legacy/internal environment variable for LangChain tracing metadata specific to the "tribrid" repo. Repo-specific tracing keys don't work well with modern LangSmith. Modern approach: us...
- **LangChain Project** (`LANGCHAIN_PROJECT`): Project name for organizing traces in LangSmith (external provider). Stored in config field tracing.langchain_project. Reserved for future integration.
- **LangChain Tracing** (`LANGCHAIN_TRACING_V2`): Reserved for future LangSmith integration (v2 tracing protocol). TriBridRAG currently captures local, in-memory request traces for UI preview and does not export traces to LangSmith yet.
- **LangGraph Final K** (`LANGGRAPH_FINAL_K`): Documents retrieved for LangGraph pipeline in /answer. Separate from retrieval FINAL_K. Higher = more context, higher cost. Typical: 10–30.
- **LangGraph Max Query Rewrites** (`LANGGRAPH_MAX_QUERY_REWRITES`): Number of query rewrites used inside the LangGraph answer pipeline (/answer). Separate from MAX_QUERY_REWRITES used by general multi-query retrieval. Higher values improve recall but increase laten...
- **LangSmith API Key** (`LANGSMITH_API_KEY`): API key for LangSmith (external provider). TriBridRAG does not currently export traces to LangSmith; the UI only checks whether this key is set in your environment.
- **LangTrace API Key** (`LANGTRACE_API_KEY`): API key for LangTrace (external provider). TriBridRAG does not currently export traces to LangTrace; the UI only checks whether this key is set in your environment.
- **LangTrace Project ID** (`LANGTRACE_PROJECT_ID`): Project identifier for LangTrace (optional). Stored in config field tracing.langtrace_project_id (and surfaced as LANGTRACE_PROJECT_ID in env exports). Reserved for future external trace export.
- **Latency P99 Threshold (s)** (`LATENCY_P99_THRESHOLD`): Threshold for alerting on tail latency: if the 99th percentile (p99) of request latency over the monitoring window exceeds this value, an alert fires. p99 focuses on the slowest 1% of requests. It’...
- **Layer Bonuses** (`repo_layerbonuses`): JSON object mapping intent types to architecture layer bonuses for smart routing. Example: {"ui": {"frontend": 0.1, "components": 0.08}, "api": {"routes": 0.1, "controllers": 0.08}}. When users ask...
- **Load History on Startup** (`CHAT_HISTORY_LOAD_ON_START`): Automatically loads and displays previous conversations when opening the Chat tab. Disable to start with a clean slate every session.
- **Log Level** (`LOG_LEVEL`): Logging verbosity level. Options: DEBUG (verbose, dev), INFO (normal, recommended), WARNING (errors + warnings only), ERROR (errors only). Higher levels reduce noise but may hide useful diagnostics.
- **Max Chunk Summaries** (`CHUNK_SUMMARIES_MAX`): Maximum number of chunk summaries to generate per corpus. Chunk summaries provide structured metadata (purpose, symbols, keywords) for each code chunk, improving retrieval quality. Higher values (2...
- **Max Response Tokens (Chat)** (`CHAT_MAX_TOKENS`): Upper bound on generated tokens for chat answers. ~4 chars ≈ 1 token. Higher values cost more and may slow responses.
- **Max tokens (response limit)** (`chat.max_tokens`): Hard cap on how long a single chat response is allowed to be. If the model hits this limit, the answer will end early even if it hasn’t finished the thought.<br><br>Raising this gives room for long...
- **MCP API Key (Optional)** (`MCP_API_KEY`): Authentication key for securing MCP server access. Stored in .env file. Leave empty to disable authentication (not recommended for production).
- **MCP HTTP Path** (`MCP_HTTP_PATH`): URL path for the HTTP MCP endpoint (default /mcp). Example: http://localhost:8013/mcp. Customize for reverse proxies or routing needs. Must match client configuration if changed.
- **MCP transports (stdio/HTTP)** (`SYS_STATUS_MCP_SERVERS`): MCP (Model Context Protocol) lets external clients (IDEs, agents, automation) call TriBridRAG tools in a standardized way.<br><br>This System Status chip lists which <span class="tt-strong">inbound...
- **Multi-Query M (RRF Constant)** (`MULTI_QUERY_M`): Constant "k" parameter in Reciprocal Rank Fusion (RRF) formula used to merge results from multiple query rewrites. RRF formula: score = sum(1 / (k + rank_i)) across all query variants. Higher M val...
- **Multi‑Query Rewrites** (`MAX_QUERY_REWRITES`): Number of LLM‑generated query variations. Each variation runs hybrid retrieval; results are merged and reranked. Higher improves recall but increases latency and API cost. Typical: 2–4.
- **Multi‑Query Rewrites** (`MQ_REWRITES`): Number of query variations to generate for improved recall. Each rewrite searches independently, then results are fused and reranked. For example, query "auth flow" might expand to "authentication ...
- **Neo4j Auto-Create Databases** (`neo4j_auto_create_databases`): Automatically create per-corpus Neo4j databases when missing (Enterprise multi-database only). When enabled, creating a corpus will create its Neo4j database automatically if it doesn't exist. When...
- **Neo4j Connection URI** (`neo4j_uri`): Neo4j database connection URI using Bolt protocol. Format: &lt;SCHEME&gt;://&lt;HOST&gt;[:&lt;PORT&gt;]. Schemes: "bolt" (no encryption), "bolt+s" (TLS with CA cert), "bolt+ssc" (TLS with self-sign...
- **Neo4j Database Mode** (`neo4j_database_mode`): Database isolation strategy: "shared" uses a single Neo4j database for all corpora (Community-compatible, requires corpus_id filtering), "per_corpus" uses separate databases per corpus (Enterprise ...
- **Neo4j Database Name** (`neo4j_database`): Neo4j database name used when database_mode is "shared". All corpora share this single database with corpus_id filtering. Default: "neo4j". For "per_corpus" mode, this is ignored and databases are ...
- **Neo4j Database Prefix** (`neo4j_database_prefix`): Prefix applied to per-corpus database names when database_mode is "per_corpus". Database names are constructed as: {prefix}{corpus_id} (sanitized). Default: "tribrid_". The prefix helps identify Tr...
- **Neo4j Password** (`neo4j_password`): Neo4j database authentication password. Required for basic authentication along with the username. Default: empty (must be set). For security, use environment variables rather than storing password...
- **Neo4j Username** (`neo4j_user`): Neo4j database authentication username. Default: "neo4j". Used for basic authentication along with the password. For Kerberos or bearer token authentication, different configuration is required. Th...
- **Netlify API Key** (`NETLIFY_API_KEY`): API key for the netlify_deploy MCP tool to trigger automated site deployments and builds. Get your personal access token from Netlify dashboard under User Settings > Applications > Personal Access ...
- **Netlify Domains** (`NETLIFY_DOMAINS`): Comma-separated list of Netlify site domains for the netlify_deploy MCP tool (e.g., "mysite.netlify.app,docs.mysite.com"). When deploying, the tool targets these specific sites. Find your site doma...
- **Normalize Scores** (`FUSION_NORMALIZE_SCORES`): Normalize scores from vector, sparse, and graph search to [0,1] range before fusion. This ensures scores from different modalities are comparable when using weighted fusion. When disabled, raw scor...
- **Out Dir Base** (`OUT_DIR_BASE`): Where retrieval looks for indices (chunks.jsonl, bm25_index/). Use ./out.noindex-shared for one index across branches so MCP and local tools stay in sync. Stores dense vectors (Qdrant), sparse BM25...
- **Path Boosts** (`repo_pathboosts`): Comma-separated directory path substrings to boost in search rankings for this repo. Examples: "src/,app/,lib/" boosts code in those directories. Use this to prioritize your main application code o...
- **Path Component Partial Match Multiplier** (`FILENAME_BOOST_PARTIAL`): Score multiplier for matches in any path component (dir name or filename prefix). Useful for queries like "auth" that should find src/auth/... files.
- **Presence Penalty** (`PRESENCE_PENALTY`): Penalizes tokens that have already appeared, encouraging the model to introduce new topics/entities. Higher values increase exploration and reduce reuse of the same concepts. Use 0.0-0.4 for factua...
- **Qdrant URL** (`QDRANT_URL`): HTTP URL for your Qdrant vector database. Used for dense vector queries during retrieval. If unavailable, retrieval still works via BM25 (sparse).
- **RAG Out Base** (`RAG_OUT_BASE`): Optional override for OUT_DIR_BASE for retrieval-specific output directory. Advanced users can use this to separate indexing output from retrieval search indices while keeping OUT_DIR_BASE for main...
- **Rate Limit Errors (per 5 min)** (`RATE_LIMIT_ERRORS_THRESHOLD`): Maximum number of rate limit errors (HTTP 429) allowed in a 5-minute window. Rate limits protect against excessive API usage and prevent cost overruns. Common sources: OpenAI API, Cohere, Voyage AI...
- **Recall intensity (next message)** (`chat_recall_intensity`): Per-message override for how aggressively to query <span class="tt-strong">Recall</span> (chat memory) on the <span class="tt-strong">next</span> send.<br><br>Options:<br>• <span class="tt-strong">...
- **Redis URL** (`REDIS_URL`): Connection string for Redis, used for LangGraph checkpoints and optional session memory. The graph runs even if Redis is down (stateless mode).
- **Repo Path (fallback)** (`REPO_PATH`): Absolute filesystem path to the active repository when repos.json is not configured. This is the directory that will be indexed for code retrieval. Use repos.json instead for multi-repo setups with...
- **Repo Path Boosts (CSV)** (`tribrid_PATH_BOOSTS`): DEPRECATED: Legacy comma-separated path boosts for the "tribrid" repository only (e.g., "app/,lib/,config/"). Repo-specific environment variables like this don't scale for multi-repo setups. Modern...
- **REPO_PATH (legacy)** (`tribrid_PATH`): DEPRECATED: Legacy environment variable for setting the repository path. This is repo-specific and only works for a repo named "tribrid". Modern approach: use REPO_PATH for single repos or configur...
- **Repos File** (`REPOS_FILE`): Path to repos.json that defines repo names, paths, keywords, path boosts, and layer bonuses used for multi-repo routing. Each repo entry includes name, path, optional keywords for boosting, path_bo...
- **Repository Keywords** (`repo_keywords`): Comma-separated keywords for query routing to this repository. When users ask questions containing these keywords, this repo is prioritized. Examples: "auth,authentication,login" or "payment,stripe...
- **Repository Path** (`repo_path`): Absolute filesystem path to the repository directory to be indexed under this logical repo name. Example: /Users/you/projects/myapp or /home/dev/backend. This directory will be scanned for code fil...
- **Repository Root Override** (`REPO_ROOT`): Override the auto-detected project root directory. TriBridRAG normally detects the repository root automatically by walking up from the current working directory to find .git or pyproject.toml. Use...
- **Reranker Auto-Reload** (`TRIBRID_RERANKER_RELOAD_ON_CHANGE`): Automatically reload the local reranker model when RERANKER_MODEL path changes during runtime (1=yes, 0=no). When enabled, the system detects model path changes and hot-reloads the new model withou...
- **Reranker Batch Size (Inference)** (`TRIBRID_RERANKER_BATCH`): Batch size used when scoring candidates during rerank. Higher values reduce latency but increase memory. If you see OOM or throttling, lower this value.
- **Reranker Blend Alpha** (`TRIBRID_RERANKER_ALPHA`): Weight of the cross-encoder reranker score during final fusion. Higher alpha prioritizes semantic pairwise scoring; lower alpha relies more on initial hybrid retrieval (BM25 + dense). Typical range...
- **Reranker Log Path** (`TRIBRID_LOG_PATH`): Directory where the reranker writes logs and training progress. Useful for monitoring and resuming experiments. Ensure the path is writable by the server process.
- **Reranker Log Path** (`TriBridRAG_LOG_PATH`): Directory where the reranker writes logs and training progress. Useful for monitoring and resuming experiments. Ensure the path is writable by the server process.
- **Reranker Max Sequence Length (Inference)** (`TRIBRID_RERANKER_MAXLEN`): Maximum token length for each (query, text) pair during live reranking. Larger values increase memory/cost and may not improve quality beyond ~256–384 tokens for code. Use higher values for long co...
- **Reranker Model Path** (`TRIBRID_RERANKER_MODEL_PATH`): Filesystem path to the active learning reranker artifact (relative paths recommended). In transformers mode this is a HuggingFace/SentenceTransformers model directory. In MLX Qwen3 learning mode th...
- **Reranker Top-N** (`TRIBRID_RERANKER_TOPN`): Maximum number of candidates to pass through the cross-encoder reranker stage during retrieval. After hybrid fusion (BM25 + dense), the top-N candidates are reranked using pairwise semantic scoring...
- **Reset Triplets Before Mining** (`TRIBRID_RERANKER_MINE_RESET`): If enabled, deletes existing mined triplets before starting a new mining run. Use with caution to avoid losing curated datasets.
- **Response Creativity (Chat)** (`CHAT_TEMPERATURE`): Controls randomness for chat answers. For code Q&A, prefer 0.0–0.3; for ideation, increase to 0.5–0.9.
- **Retrieval Confidence** (`CHAT_CONFIDENCE`): Show a normalized confidence score (0–1) alongside answers to help judge reliability. Scores reflect retrieval confidence, not model certainty.
- **RRF k Parameter** (`FUSION_RRF_K`): Smoothing constant for Reciprocal Rank Fusion. Higher values (80-120) give more weight to top-ranked results, lower values (40-60) distribute weight more evenly across ranks. The original RRF paper...
- **Save Chat Messages** (`CHAT_HISTORY_ENABLED`): When enabled, messages are persisted to browser localStorage and restored on reload. Disable for ephemeral sessions or shared devices.
- **Semantic Synonyms Expansion** (`USE_SEMANTIC_SYNONYMS`): Expands queries with curated domain synonyms and abbreviations (e.g., auth → authentication, oauth, jwt). Complements LLM rewrites. Configure in data/semantic_synonyms.json.
- **Sparse Search Enabled** (`SPARSE_SEARCH_ENABLED`): Enable or disable sparse (BM25 keyword) search. When enabled, queries use lexical matching to find chunks containing exact keywords, variable names, function names, and error codes. When disabled, ...
- **Sparse Search Top-K** (`SPARSE_SEARCH_TOP_K`): Number of candidate results to retrieve from BM25 sparse search before fusion. Higher values (75-150) improve recall for exact keyword matches (variable names, function names, error codes) but incr...
- **Sparse Weight** (`FUSION_SPARSE_WEIGHT`): Weight assigned to sparse (BM25) search results in weighted fusion mode. Higher values (0.4-0.6) favor keyword matches, lower values (0.2-0.3) reduce keyword influence. Weights must sum to ~1.0 wit...
- **Stream Timeout (seconds)** (`CHAT_STREAM_TIMEOUT`): Maximum time in seconds to wait for a streaming chat response to complete. If the stream doesn't finish within this time, the connection will be closed. Increase for complex queries that require lo...
- **Streaming responses** (`ui.chat_streaming_enabled`): Streams tokens as they’re generated (instead of waiting for the full answer). This makes chat feel faster and lets you start reading immediately.<br><br>If streaming fails for a provider, the chat ...
- **Sustained Frequency Duration (minutes)** (`ENDPOINT_SUSTAINED_DURATION`): How long the high call frequency must be sustained before triggering an alert. Prevents false positives from legitimate bursts. For example, if frequency threshold is 20 calls/min and duration is 2...
- **Synonyms File Path** (`TRIBRID_SYNONYMS_PATH`): Custom path to the semantic synonyms JSON file. Defaults to data/semantic_synonyms.json if empty. Use this to point to a repository-specific or custom synonym dictionary. The file should contain a ...
- **Synonyms File Path** (`TriBridRAG_SYNONYMS_PATH`): Custom path to the semantic synonyms JSON file. Defaults to data/semantic_synonyms.json if empty. Use this to point to a repository-specific or custom synonym dictionary. The file should contain a ...
- **System prompt (base)** (`chat.system_prompt_base`): This is the <span class="tt-strong">legacy</span> system prompt path. TriBridRAG normally chooses one of the four “state” prompts (Direct / RAG / Recall / RAG+Recall) based on what context is actua...
- **System Prompts** (`SYSTEM_PROMPTS_SUBTAB`): Edit LLM system prompts that control RAG pipeline behavior. These prompts are used for query expansion, chat responses, semantic card generation, code enrichment, and eval analysis. Changes are sav...
- **Table Name** (`TABLE_NAME`): Optional override for the pgvector table name where vectors are stored. Defaults to code_chunks_{REPO}. Set this if you maintain multiple profiles, A/B test embedding models, or run parallel indexi...
- **Temperature (no retrieval)** (`chat.temperature_no_retrieval`): Controls generation when the message is answered in “Direct chat” — no corpora checked (no retrieval context). This is intentionally separate so direct conversation can feel more fluid without maki...
- **Temperature (with retrieval)** (`chat.temperature`): Controls how “random” generation is when <span class="tt-strong">any sources are enabled</span> (RAG corpora and/or Recall). Retrieval decides what context is shown; temperature decides how strictl...
- **Thinking Budget Tokens** (`CHAT_THINKING_BUDGET_TOKENS`): Maximum number of tokens allocated for the model's internal reasoning/thinking process when using thinking-enabled models like Anthropic Claude with extended thinking. Higher budgets allow deeper r...
- **Thread ID** (`THREAD_ID`): Unique identifier for conversation session state in LangGraph checkpoints or CLI chat. Use a stable value (e.g., "session-123", user email, UUID) to preserve chat history and context across runs. D...
- **Timeout Errors (per 5 min)** (`TIMEOUT_ERRORS_THRESHOLD`): Maximum number of timeout errors allowed in a 5-minute window before triggering an alert. Timeout errors indicate requests that took too long and were forcibly terminated. Common causes: slow LLM A...
- **Trace Retention** (`TRACE_RETENTION`): Number of traces to retain in the in-memory ring buffer (10-500). Higher values preserve more history for debugging; lower values use less memory.
- **Trace Sampling Rate** (`TRACE_SAMPLING_RATE`): Percentage of requests to trace with LangSmith/observability (0.0-1.0). 1.0 = trace everything (100%), 0.1 = trace 10% of requests, 0.0 = no tracing. Lower sampling reduces LangSmith costs and over...
- **Tracing Enabled** (`TRACING_ENABLED`): Enable TriBridRAG request tracing. This records an in-memory per-request event trace (used by the UI “Routing Trace” preview) for debugging routing/retrieval decisions and latency. This does not ex...
- **Tracing Mode** (`TRACING_MODE`): Controls tracing behavior. Options: "off" (disable tracing), "local" (local-only), "langsmith" (local traces + reserved for future LangSmith export). Alias: "none" is normalized to "off".
- **Transformers: trust_remote_code** (`TRANSFORMERS_TRUST_REMOTE_CODE`): SECURITY WARNING: Set to "true" only if you completely trust the model source. Allows HuggingFace Transformers to execute arbitrary Python code from model repositories for custom architectures. Mal...
- **Triplet Mining Mode** (`TRIBRID_RERANKER_MINE_MODE`): Strategy for mining training triplets: random, semi‑hard, or hard negatives. Harder negatives improve discriminative power but may be noisier and slower to mine.
- **Triplets Dataset Path** (`TRIBRID_TRIPLETS_PATH`): Path to mined triplets used for training the Learning Reranker. Triplets contain (query, positive, negative) examples. Keep under version control or in a reproducible data store.
- **Triplets Dataset Path** (`TriBridRAG_TRIPLETS_PATH`): Path to mined triplets used for training the Learning Reranker. Triplets contain (query, positive, negative) examples. Keep under version control or in a reproducible data store.
- **Triplets Min Count** (`TRIPLETS_MIN_COUNT`): Minimum number of training triplets (query, positive_doc, negative_doc) required to proceed with reranker training. Acts as a data quality gate - training with too few examples leads to severe over...
- **Triplets Mine Mode** (`TRIPLETS_MINE_MODE`): Strategy for mining negative examples when constructing training triplets from query logs and feedback. Negative examples are crucial for learning to rank - they teach the model what NOT to retriev...
- **Vector Search Enabled** (`VECTOR_SEARCH_ENABLED`): Enable or disable vector (dense semantic) search using pgvector. When enabled, queries use embedding similarity to find semantically related chunks. When disabled, only sparse (BM25) and graph sear...
- **Vector Search Top-K** (`VECTOR_SEARCH_TOP_K`): Number of candidate results to retrieve from pgvector vector search before fusion. Higher values (75-150) improve recall for semantic matches but increase query latency and memory usage. Lower valu...
- **Vector Similarity Threshold** (`VECTOR_SIMILARITY_THRESHOLD`): Minimum cosine similarity score (0.0-1.0) required to include a vector search result. Results below this threshold are filtered out before fusion. 0.0 = no threshold (all results included). Higher ...
- **Vector Weight** (`FUSION_VECTOR_WEIGHT`): Weight assigned to vector (pgvector) search results in weighted fusion mode. Higher values (0.5-0.7) favor semantic matches, lower values (0.2-0.4) reduce semantic influence. Weights must sum to ~1...
- **Vendor Mode** (`VENDOR_MODE`): Controls scoring preference for your code vs third-party library code during reranking. "prefer_first_party" (recommended) boosts your app code (+0.06) and penalizes node_modules/vendor libs (-0.08...
- **Vendor Penalty** (`VENDOR_PENALTY`): Score penalty (negative bonus) applied to third-party library code (node_modules, vendor/, site-packages/, etc.) during reranking when VENDOR_MODE is set to prefer_first_party. Helps prioritize you...
- **Vision enabled** (`chat.multimodal.vision_enabled`): Enables image attachments for chat messages. When on, the UI can send images along with your message to a vision-capable model.<br><br>If your selected model/provider doesn’t support vision, images...
- **Voyage API Key** (`VOYAGE_API_KEY`): API key for Voyage AI embeddings when EMBEDDING_TYPE=voyage.

### generation

- **Anthropic API Key** (`ANTHROPIC_API_KEY`): API key for Anthropic models (Claude family: claude-3-5-sonnet, claude-3-opus, claude-instant). Required when using Claude models for generation. Get your key from Anthropic Console under Account S...
- **Chat-Specific Model** (`GEN_MODEL_CHAT`): Override model for Chat interface only. Leave empty to use GEN_MODEL. Useful for using a different model (e.g., faster/cheaper) specifically for interactive chat.
- **CLI Channel Model** (`GEN_MODEL_CLI`): Override GEN_MODEL for CLI chat sessions only. Allows using different models for terminal vs web interface - e.g., faster models for CLI iteration, higher quality for production GUI. Useful for dev...
- **Default Chat Model** (`CHAT_DEFAULT_MODEL`): Default LLM model used for chat when not overridden per-request. Common options: gpt-4o-mini (fast/cheap), gpt-4o (balanced), claude-3-5-sonnet (high quality), or local Ollama models. Per-request m...
- **Default Response Creativity** (`GEN_TEMPERATURE`): Global default temperature for generation. 0.0 = deterministic; small values (0.04-0.2) add slight variation in prose. Use per-model tuning for creative tasks vs. code answers.
- **Enrichment Model** (`ENRICH_MODEL`): Specific model name for code enrichment when ENRICH_BACKEND is set. For OpenAI: "gpt-4o-mini" (recommended, cheap), "gpt-4o" (higher quality, costly). For Ollama: specify via ENRICH_MODEL_OLLAMA in...
- **Enrichment Model (Ollama)** (`ENRICH_MODEL_OLLAMA`): Ollama model name for code enrichment when ENRICH_BACKEND=ollama. Recommended: "qwen2.5-coder:7b" (fast, code-focused), "deepseek-coder:6.7b" (excellent code understanding), "codellama:13b" (high q...
- **Generation Max Retries** (`GEN_RETRY_MAX`): Number of retry attempts for failed LLM API calls due to rate limits, network errors, or transient failures. Higher values improve reliability but increase latency on failures. Typical: 2-3 retries.
- **Generation Model** (`GEN_MODEL`): Answer model. Local: qwen3-coder:14b via Ollama. Cloud: gpt-4o-mini, etc. Larger models cost more and can be slower; smaller ones are faster/cheaper.
- **Generation Timeout** (`GEN_TIMEOUT`): Maximum seconds to wait for LLM response before timing out. Prevents hanging on slow models or network issues. Increase for large models or slow connections. Typical: 30-120 seconds.
- **HTTP Channel Model** (`GEN_MODEL_HTTP`): Override GEN_MODEL specifically for HTTP API requests (GUI, external API calls). Useful for serving different models to different channels - e.g., use gpt-4o for production HTTP but qwen-coder loca...
- **Local Request Timeout (seconds)** (`OLLAMA_REQUEST_TIMEOUT`): Maximum total time to wait for a single local (Ollama) generation request to complete. Increase for long answers; decrease to fail fast on slow models or poor connectivity.
- **Local Stream Idle Timeout (seconds)** (`OLLAMA_STREAM_IDLE_TIMEOUT`): Maximum idle time allowed between streamed chunks from local (Ollama). If no tokens arrive within this window, the request aborts to prevent hanging streams.
- **Max Tokens** (`GEN_MAX_TOKENS`): Maximum number of tokens the LLM can generate in a single response. Higher values allow longer answers but increase cost and latency. Typical: 512-1024 for concise answers, 2048-4096 for detailed e...
- **MCP Channel Model** (`GEN_MODEL_MCP`): Override GEN_MODEL for MCP tool invocations only. Use a lighter/cheaper model for MCP tools since tool calls are typically simpler than complex reasoning. Example: gpt-4o-mini for MCP, gpt-4o for m...
- **Ollama URL** (`OLLAMA_URL`): Local inference endpoint for Ollama running on your machine (e.g., http://127.0.0.1:11434/api). Used when GEN_MODEL targets a local model like llama2, mistral, qwen, or neural-chat. Requires Ollama...
- **OpenAI API Key** (`OPENAI_API_KEY`): API key used for OpenAI-based embeddings and/or generation.
- **OpenAI Base URL** (`OPENAI_BASE_URL`): ADVANCED: Override the OpenAI API base URL for OpenAI-compatible endpoints. Use cases: local inference servers (LM Studio, vLLM, text-generation-webui), Azure OpenAI (https://YOUR_RESOURCE.openai.a...
- **System prompt suffix: RAG (legacy)** (`chat.system_prompt_rag_suffix`): This text is appended to the legacy base prompt <span class="tt-strong">only when code retrieval (RAG) context is present</span> and the selected four-state system prompt is blank (fallback mode). ...
- **System prompt suffix: Recall (legacy)** (`chat.system_prompt_recall_suffix`): This text is appended to the legacy base prompt <span class="tt-strong">only when Recall context is present</span> and the selected four-state system prompt is blank (fallback mode). It’s meant to ...
- **System prompt: Direct (no context)** (`chat.system_prompt_direct`): Used when the message is answered <span class="tt-strong">without any retrieval context</span> — no code corpora results and no Recall snippets.<br><br>In this state the model only sees the user’s ...
- **System prompt: RAG + Recall** (`chat.system_prompt_rag_and_recall`): Used when both code context and Recall context are present. The model receives:<br>• <span class="mono">&lt;rag_context&gt;...&lt;/rag_context&gt;</span> (code snippets)<br>• <span class="mono">&lt...
- **System prompt: RAG only** (`chat.system_prompt_rag`): Used when code corpora are queried and retrieval returns results. The model receives code snippets inside <span class="mono">&lt;rag_context&gt;...&lt;/rag_context&gt;</span>.<br><br>Each snippet i...
- **System prompt: Recall only** (`chat.system_prompt_recall`): Used when Recall (chat memory) is queried and returns results, but no code corpora results are present. The model receives conversation snippets inside <span class="mono">&lt;recall_context&gt;...&...
- **Top-P (Nucleus Sampling)** (`GEN_TOP_P`): Controls randomness via nucleus sampling (0.0-1.0). Lower values (0.1-0.5) make output more focused and deterministic. Higher values (0.9-1.0) increase creativity and diversity. Recommended: 0.9 fo...
- **Voyage Embedding Model** (`VOYAGE_MODEL`): Voyage AI embedding model when EMBEDDING_TYPE=voyage. Options: "voyage-code-2" (1536 dims, optimized for code, recommended), "voyage-3" (1024 dims, general-purpose, fast), "voyage-3-lite" (512 dims...

### indexing

- **Parquet Extract Max Cell Chars** (`PARQUET_EXTRACT_MAX_CELL_CHARS`): Maximum characters per Parquet cell when converting to text (best-effort). Long cells are truncated to keep output bounded.
- **Parquet Extract Max Chars** (`PARQUET_EXTRACT_MAX_CHARS`): Maximum total characters to extract from a single Parquet file during indexing (best-effort). Extraction stops once this limit is reached.
- **Parquet Extract Max Rows** (`PARQUET_EXTRACT_MAX_ROWS`): Maximum number of rows to read from a single Parquet file during indexing (best-effort). Prevents huge datasets from consuming excessive memory/time.
- **Parquet Include Column Names** (`PARQUET_EXTRACT_INCLUDE_COLUMN_NAMES`): When enabled, extracted Parquet text includes column headers like `[column]` to preserve context across fields.
- **Parquet Text Columns Only** (`PARQUET_EXTRACT_TEXT_COLUMNS_ONLY`): When enabled, only string/text-like columns are extracted from Parquet files (best-effort). Disabling may include numeric/structured columns as text.

### infrastructure

- **All Containers** (`DOCKER_ALL_CONTAINERS`): Complete list of Docker containers on this host, including running, stopped, and paused containers. Use this view to manage container lifecycle (start, stop, restart, pause, remove) and view logs f...
- **Container Action Timeout** (`DOCKER_CONTAINER_ACTION_TIMEOUT`): Maximum seconds to wait for container start/stop/restart operations. Containers with complex startup sequences or cleanup hooks may need higher values. If container actions timeout, increase this. ...
- **Container List Timeout** (`DOCKER_CONTAINER_LIST_TIMEOUT`): Maximum seconds to wait when listing all Docker containers. Increase if you have many containers (100+) or slow Docker API response. Range: 1-60 seconds.
- **Docker Settings** (`DOCKER_SETTINGS`): Configuration settings for Docker timeouts and log behavior. These settings control how long TriBridRAG waits for Docker operations and how logs are displayed. Adjust these if you experience timeou...
- **Docker Status** (`DOCKER_STATUS`): Real-time status of the Docker daemon connection. Shows whether TriBridRAG can communicate with Docker to manage containers. If status is unhealthy, ensure Docker Desktop is running or the Docker d...
- **Docker Status Timeout** (`DOCKER_STATUS_TIMEOUT`): Maximum seconds to wait when checking Docker daemon status. Increase if your Docker host is slow to respond or under heavy load. If health checks timeout frequently, raise this value. Range: 1-30 s...
- **Editor Port** (`EDITOR_PORT`): TCP port for code editor service. Default: varies by editor. Must not conflict with other services (PORT, MCP_HTTP_PORT, PROMETHEUS_PORT).
- **Excluded Extensions** (`INDEX_EXCLUDED_EXTS`): Comma-separated file extensions to skip during indexing (e.g., ".png,.jpg,.pdf,.zip"). Prevents indexing binary files, images, or non-code assets. Reduces index size and improves relevance.
- **Grafana Base URL** (`GRAFANA_BASE_URL`): Base URL for Grafana dashboard server (e.g., http://localhost:3000). Used for embedded dashboard iframes in GUI and direct links to monitoring dashboards.
- **HTTP Port** (`PORT`): TCP port for the HTTP server that serves the GUI and API endpoints when running serve_rag. Default: 8012. Change if port 8012 is already in use by another service (common conflict: development serv...
- **Include Log Timestamps** (`DOCKER_LOGS_TIMESTAMPS`): Whether to include timestamps in Docker log output. Timestamps help correlate events across containers but add visual noise. Set to 1 to show timestamps, 0 to hide them.
- **Index Max Workers** (`INDEX_MAX_WORKERS`): Maximum number of parallel workers used during indexing. Increase to speed up indexing on multi‑core machines; decrease if you observe system contention. A good starting point is CPU cores − 1.
- **Index Profiles** (`INDEX_PROFILES`): Preset configurations for common workflows: shared (BM25‑only, fast), full (BM25 + embeddings, best quality), dev (small subset). Profiles change multiple parameters at once to match your goal.
- **Index Readiness** (`DASHBOARD_INDEX_PANEL`): Live embedding config, indexing cost, and storage requirements pulled directly from /api/index/status. Updates automatically every 30 seconds and mirrors the legacy GUI layout exactly.
- **Indexing Logs Terminal** (`INDEX_LOGS_TERMINAL`): Open the sliding terminal to stream raw indexer output with the exact repo/skip_dense/enrich settings used for the run.
- **Indexing Process** (`INDEXING_PROCESS`): Indexing prepares your code for retrieval: it chunks files, builds a BM25 sparse index, optionally generates dense embeddings, and writes vectors to Qdrant. Re‑run after significant code changes to...
- **Indexing Workers** (`INDEXING_WORKERS`): Number of parallel worker threads for CPU-intensive indexing tasks (file parsing, chunking, BM25 indexing). Higher values (4-16) utilize multi-core CPUs better and speed up indexing significantly. ...
- **Infrastructure Down Timeout** (`DOCKER_INFRA_DOWN_TIMEOUT`): Maximum seconds to wait when stopping TriBridRAG infrastructure services. Containers with data persistence may need time to flush to disk. If infra down fails, increase this value. Range: 10-120 se...
- **Infrastructure Services** (`DOCKER_INFRASTRUCTURE_SERVICES`): TriBridRAG infrastructure containers that power the RAG engine. Includes Postgres (storage + pgvector), Neo4j (graph), Grafana (monitoring), Loki (log aggregation), Prometheus (metrics), and Alertm...
- **Infrastructure Up Timeout** (`DOCKER_INFRA_UP_TIMEOUT`): Maximum seconds to wait when starting TriBridRAG infrastructure services (Postgres, Neo4j, Grafana, Loki, etc.) via docker-compose. First-time startup may pull images and take longer. If infra up f...
- **LangTrace API Host** (`LANGTRACE_API_HOST`): LangTrace API endpoint host (optional). Stored in config field tracing.langtrace_api_host (and surfaced as LANGTRACE_API_HOST in env exports). Reserved for future external trace export.
- **Log Lines to Tail** (`DOCKER_LOGS_TAIL`): Number of log lines to display when viewing container logs. Higher values show more history but may slow down log retrieval. Use 50-100 for quick checks, 500-1000 for debugging. Range: 10-1000 lines.
- **Max Indexable File Size** (`MAX_INDEXABLE_FILE_SIZE`): Maximum file size in bytes that will be indexed. Files larger than this limit are skipped during indexing to prevent memory issues and avoid indexing large binary or generated files. Default is 2MB...
- **MCP HTTP Host** (`MCP_HTTP_HOST`): Bind address for the HTTP MCP server (fast transport). Use 0.0.0.0 to listen on all interfaces, 127.0.0.1 for localhost only, or a specific IP like 192.168.1.100 for LAN access. MCP (Model Context ...
- **MCP HTTP Port** (`MCP_HTTP_PORT`): TCP port for HTTP MCP server (default 8013). Must not conflict with other services. Use ports 1024+ without special permissions. MCP enables fast, stateless communication for multi-client scenarios.
- **MCP Server URL** (`MCP_SERVER_URL`): Complete URL for the HTTP MCP server. Combines host, port, and path into a single endpoint that MCP clients connect to.
- **Neo4j Connection URI** (`NEO4J_URI`): Connection URI for Neo4j graph database. Used for entity relationships, graph traversal, and community detection in tri-brid search. Format: bolt://host:7687 or neo4j://host:7687. Enables graph-bas...
- **Per-Repository Indexing Configuration** (`PER_REPO_INDEXING`): Override global indexing settings per repo. Enables optimization for different codebases. Scenarios: docs repos (larger chunks 1500-2000, stemmer), dense code (smaller chunks 500-800, whitespace), ...
- **PostgreSQL pgvector URL** (`POSTGRES_URL`): Connection URL for PostgreSQL with pgvector extension. Used for dense vector storage and similarity search. Format: postgresql://user:pass@host:port/db. The pgvector extension enables efficient vec...
- **Preserve Imports** (`PRESERVE_IMPORTS`): Include import/require statements in chunks even if they fall below MIN_CHUNK_CHARS threshold (1=yes, 0=no). When enabled, import blocks become searchable, helping users find dependency usage and m...
- **Prometheus Port** (`PROMETHEUS_PORT`): TCP port for Prometheus metrics endpoint. Exposes /metrics for scraping by Prometheus or Grafana. Default: 9090. Change to avoid conflicts with existing monitoring tools.
- **Server Host** (`HOST`): Network interface for the HTTP server to bind to when running serve_rag. Use 0.0.0.0 for all interfaces (accessible from network), 127.0.0.1 for localhost only (secure, dev mode).
- **Validation Error** (`INDEX_VALIDATION_ERROR`): Configuration issues that must be fixed before indexing can proceed. Common errors: embedding dimension mismatch with existing index, missing API keys for cloud providers, chunk overlap exceeding c...
- **Validation Warning** (`INDEX_VALIDATION_WARNING`): Configuration may reduce retrieval quality but indexing can still proceed. Common warnings: skip dense vectors enabled (BM25-only mode), very large chunk sizes (>2000), small chunks with AST strate...

### reranking

- **Active Reranker** (`RERANKER_ACTIVE`): Route reranking to local vs cloud. • local/learning — on-host (includes TriBridRAG learning reranker) • cloud — uses provider/model from models.json • none/off — disables rerank. If cloud is select...
- **Cloud Model** (`RERANKER_CLOUD_MODEL`): Provider-scoped rerank model id from models.json. Examples: rerank-3.5 (cohere), rerank-2 (voyage), or any custom id you add. Model list comes from models.json; add entries there to surface more op...
- **Cloud Provider (models.json)** (`RERANKER_PROVIDER`): Provider id for cloud reranking, loaded dynamically from models.json via /api/models. Examples: cohere, voyage, openai, or any custom provider you add. No hardcoded lists; extend models.json to exp...
- **Cloud Rerank Provider** (`RERANKER_CLOUD_PROVIDER`): When RERANKER_MODE=cloud, specifies which API provider to use for reranking. Options: cohere, voyage, jina. Each provider has different pricing and model options—see models.json for available model...
- **Cloud Reranker Top-N** (`RERANKER_CLOUD_TOP_N`): Maximum number of candidates to send to cloud reranking APIs (Cohere, Voyage, Jina). Cloud rerankers have rate limits and per-request pricing, so this setting is separate from the local reranker to...
- **Cohere Rerank Calls (calls/min)** (`COHERE_RERANK_CALLS`): Alert when Cohere reranking API is called this many times per minute. Reranking is expensive ($1-2 per 1M tokens) and high call rates can quickly increase costs. Normal usage: 5-10 calls/min. If th...
- **Cohere Rerank Model** (`COHERE_RERANK_MODEL`): Cohere rerank model name (e.g., rerank-3.5). Check the provider docs for the latest list and pricing.
- **Learning Reranker Backend** (`LEARNING_RERANKER_BACKEND`): Backend used when RERANKER_MODE="learning". auto uses MLX Qwen3 LoRA only on macOS Apple Silicon when MLX dependencies are installed; all other environments use transformers. Use transformers to fo...
- **Learning Reranker Base Model** (`LEARNING_RERANKER_BASE_MODEL`): Base model identifier to fine-tune FROM when using the MLX Qwen3 learning backend (e.g. Qwen/Qwen3-Reranker-0.6B). Training produces a LoRA adapter artifact written under TRIBRID_RERANKER_MODEL_PAT...
- **Learning Reranker Grad Accum Steps** (`LEARNING_RERANKER_GRAD_ACCUM_STEPS`): Number of micro-batches to accumulate gradients over before applying one optimizer update when training the MLX Qwen3 learning reranker. This increases effective batch size without increasing memor...
- **Learning Reranker Idle Unload** (`LEARNING_RERANKER_UNLOAD_AFTER_SEC`): If >0, unload the MLX Qwen3 reranker model from memory after this many seconds of inactivity. Set to 0 to keep the model resident (faster first rerank, higher memory use).
- **Learning Reranker LoRA Alpha** (`LEARNING_RERANKER_LORA_ALPHA`): LoRA scaling (alpha) for MLX Qwen3 fine-tuning. Effective LoRA scale is alpha/rank. Typical: 16–64; start at 32.
- **Learning Reranker LoRA Dropout** (`LEARNING_RERANKER_LORA_DROPOUT`): LoRA dropout probability for MLX Qwen3 fine-tuning. Small dropout (0.0–0.1) can reduce overfitting on small mined datasets. Start at 0.05.
- **Learning Reranker LoRA Rank** (`LEARNING_RERANKER_LORA_RANK`): LoRA rank (r) for MLX Qwen3 fine-tuning. Higher rank increases adapter capacity (and training/inference cost) and can improve quality with enough data. Typical: 8–32; start at 16.
- **Learning Reranker LoRA Target Modules** (`LEARNING_RERANKER_LORA_TARGET_MODULES`): Comma-separated list of module names to apply LoRA to for the MLX Qwen3 learning backend (typical attention projections: q_proj,k_proj,v_proj,o_proj). Targeting more modules increases capacity but ...
- **Learning Reranker Negative Ratio** (`LEARNING_RERANKER_NEGATIVE_RATIO`): When converting mined triplets into labeled (query, document) pairs for pairwise training, use up to this many negatives per positive. Higher ratios can improve discrimination but increase training...
- **Learning Reranker Promotion Epsilon** (`LEARNING_RERANKER_PROMOTE_EPSILON`): Minimum dev-metric improvement required to auto-promote a newly trained learning reranker artifact over the active baseline. Use a small epsilon (e.g., 0.002) to avoid promoting on noise.
- **Learning Reranker Promotion Gate** (`LEARNING_RERANKER_PROMOTE_IF_IMPROVES`): If enabled (1), training only promotes the newly trained artifact to TRIBRID_RERANKER_MODEL_PATH if the primary dev metric improves over the active baseline by at least LEARNING_RERANKER_PROMOTE_EP...
- **Local Reranker (HF)** (`RERANKER_MODEL`): HuggingFace model name or path for local reranking when RERANK_BACKEND=local or hf. Common options: "cross-encoder/ms-marco-MiniLM-L-6-v2" (fast, good quality), "BAAI/bge-reranker-base" (higher qua...
- **Local Reranker Model** (`RERANKER_LOCAL_MODEL`): When RERANKER_MODE=local, specifies the model to load. Can be: • A HuggingFace model ID (e.g., BAAI/bge-reranker-v2-m3) • A local filesystem path (e.g., /models/my-reranker) • Any sentence-transfor...
- **Rerank Backend** (`RERANK_BACKEND`): Reranks fused candidates for better ordering. • cohere — best quality, paid (COHERE_API_KEY) • local/hf — no cost (ensure model installed) Disable only to save cost.
- **Rerank Snippet Length** (`RERANK_INPUT_SNIPPET_CHARS`): Maximum characters from each candidate chunk sent to the reranker. Keeps payloads within provider limits and focuses scoring on the most relevant prefix. Typical range: 400-1200 chars. Use 400-600 ...
- **Reranker Auto-Reload** (`TriBridRAG_RERANKER_RELOAD_ON_CHANGE`): Automatically reload the local reranker model when RERANKER_MODEL path changes during runtime (1=yes, 0=no). When enabled, the system detects model path changes and hot-reloads the new model withou...
- **Reranker Backend** (`RERANKER_BACKEND`): Choose the reranking provider to reorder retrieved results by semantic relevance (cross-encoder). Options typically include Cohere Rerank, the built‑in TriBridRAG Learning Reranker, or none. Rerank...
- **Reranker Blend Alpha** (`TriBridRAG_RERANKER_ALPHA`): Weight of the cross-encoder reranker score during final fusion. Higher alpha prioritizes semantic pairwise scoring; lower alpha relies more on initial hybrid retrieval (BM25 + dense). Typical range...
- **Reranker Max Sequence Length (Inference)** (`TriBridRAG_RERANKER_MAXLEN`): Maximum token length for each (query, text) pair during live reranking. Larger values increase memory/cost and may not improve quality beyond ~256–384 tokens for code. Use higher values for long co...
- **Reranker Mode** (`RERANKER_MODE`): Controls which reranking approach is used. Four options: • none: Disabled—BM25 + vector fusion only (no reranker scoring). • local: A local HuggingFace reranker (cross-encoder) you provide (BGE, Ji...
- **Reranker Model Path** (`TriBridRAG_RERANKER_MODEL_PATH`): Filesystem path to the trained reranker model checkpoint directory (relative paths recommended). The service loads weights from this path on startup or when reloaded.
- **Reranker Timeout** (`RERANKER_TIMEOUT`): Timeout (seconds) for cloud reranker HTTP calls. Larger timeouts reduce false failures on slow providers; smaller timeouts fail fast when endpoints are slow or unreachable. Applies only to cloud ba...
- **Reranker Top-N** (`TriBridRAG_RERANKER_TOPN`): Maximum number of candidates to pass through the cross-encoder reranker stage during retrieval. After hybrid fusion (BM25 + dense), the top-N candidates are reranked using pairwise semantic scoring...
- **Reranker Train Max Length** (`RERANKER_TRAIN_MAX_LENGTH`): Maximum token length for reranker training examples. Longer sequences may improve context but require more memory and training time. Typical range: 256–1024.
- **Reset Triplets Before Mining** (`TriBridRAG_RERANKER_MINE_RESET`): If enabled, deletes existing mined triplets before starting a new mining run. Use with caution to avoid losing curated datasets.
- **Training Epochs** (`RERANKER_TRAIN_EPOCHS`): Number of full passes over the training triplets for the learning reranker. More epochs can improve quality but risk overfitting when data is small. Start with 1–2 and increase as your mined datase...
- **Training Learning Rate** (`RERANKER_TRAIN_LR`): Learning rate for the cross-encoder optimizer during fine-tuning. This controls the size of weight updates during gradient descent. Standard range for cross-encoder fine-tuning is 1e-6 to 5e-5. Hig...
- **Training Max Sequence Length** (`RERANKER_TRAIN_MAXLEN`): Token limit for the cross-encoder during training. Longer sequences increase memory quadratically. If training fails with OOM (-9) under Docker/Colima, set 128–256. Sequences longer than the limit ...
- **Triplet Mining Mode** (`TriBridRAG_RERANKER_MINE_MODE`): Strategy for mining training triplets: random, semi\u2011hard, or hard negatives. Harder negatives improve discriminative power but may be noisier and slower to mine.
- **Voyage Rerank Model** (`VOYAGE_RERANK_MODEL`): Voyage AI reranker model name when RERANK_BACKEND=voyage. Current option: "rerank-2" (latest, best quality). Voyage rerankers are cross-encoders that score (query, document) pairs for precise relev...
- **Warmup Ratio** (`RERANKER_WARMUP_RATIO`): Fraction of total training steps to use for linear learning rate warmup. During warmup, the learning rate gradually increases from 0 to your target RERANKER_TRAIN_LR, preventing early training inst...

### retrieval

- **BM25 b (Length Normalization)** (`BM25_B`): BM25 length-normalization parameter. Controls how strongly sparse (keyword) scoring penalizes long chunks compared to short chunks. b = 0.0 means no length penalty (a long chunk can score highly si...
- **BM25 k1 (Term Saturation)** (`BM25_K1`): BM25 term-frequency saturation parameter. Controls how much repeated occurrences of a query term within the same chunk increase the sparse score. Low k1 makes BM25 behave closer to “binary” matchin...
- **BM25 Stemmer Language** (`BM25_STEMMER_LANG`): Language for stemming/normalization in BM25 sparse indexing. Common values: "en" (English - default), "multilingual" (multiple languages), "none" (disable stemming). Stemming reduces words to root ...
- **BM25 Stopwords Language** (`BM25_STOPWORDS_LANG`): Language code used to select the stopword list for sparse (BM25/FTS) tokenization. Stopwords are extremely common words that are ignored to reduce noise (e.g., “the”, “and”, “of”). For code-heavy c...
- **BM25 Tokenizer** (`BM25_TOKENIZER`): Tokenization strategy for BM25 sparse index. Controls how code text is split into searchable terms. Options: "stemmer" (Porter stemming, normalizes word forms like "running" → "run"), "whitespace" ...
- **BM25 Vocabulary Preview** (`BM25_VOCAB_PREVIEW`): Inspect tokenized vocabulary from BM25 sparse index. Shows term frequencies for debugging. Use cases: verify code identifiers preserved, check stemmer behavior, identify noise terms, debug zero-res...
- **BM25 Weight (Hybrid Fusion)** (`BM25_WEIGHT`): Weight assigned to BM25 (sparse lexical) scores during hybrid search fusion. BM25 excels at exact keyword matches - variable names, function names, error codes, technical terms. Higher weights (0.5...
- **Card Search Enabled** (`CARD_SEARCH_ENABLED`): Enable card-based boosting during retrieval to surface relevant code modules and features. When enabled, the system loads summary cards (high-level descriptions of modules/classes/features) and boo...
- **Chunk Summary Bonus** (`CHUNK_SUMMARY_BONUS`): Additive score bonus applied to results that come from chunk_summary-based retrieval. Chunk summaries are short, structured descriptions of code chunks (purpose, key symbols, keywords) and can matc...
- **Chunk Summary Search** (`CHUNK_SUMMARY_SEARCH_ENABLED`): Enable an additional retrieval pass that searches over each chunk’s chunk_summary (LLM-generated metadata such as purpose, key symbols, and keywords) instead of only raw chunk text. This can improv...
- **Community Detection** (`INCLUDE_COMMUNITIES`): Enable community-based expansion in graph search. Communities are clusters of closely related code entities detected using algorithms like Louvain. Helps find related functionality even without dir...
- **Dedup by** (`DEDUP_BY`): How to deduplicate fused retrieval results.<br><br><b>chunk_id</b>: keep distinct chunks (best default).<br><b>file_path</b>: at most one result per file (improves diversity but disables neighbor-w...
- **Deep on explicit reference** (`chat.recall_gate.deep_on_explicit_reference`): When enabled, the gate escalates to <span class="tt-strong">deep</span> mode if your message explicitly references earlier conversation (“we discussed…”, “you mentioned…”, “last time…”, “remember w...
- **Deep recency weight** (`chat.recall_gate.deep_recency_weight`): Recency bias for deep Recall. Deep mode is often about retrieving decisions and conclusions from the recent thread, so this usually runs higher than the standard value.<br><br>Raise it if “what did...
- **Deep top_k** (`chat.recall_gate.deep_top_k`): How many Recall snippets to include in <span class="tt-strong">deep</span> mode.<br><br>Deep is designed for “look back” questions. Larger values help capture multi-message decisions and longer thr...
- **Default intensity** (`chat.recall_gate.default_intensity`): The “no strong signal” fallback. When the gate doesn’t see a clear reason to skip/light/deep, it uses this intensity.<br><br>Choosing a default is mostly about what you want normal chat to feel lik...
- **Enable MMR** (`ENABLE_MMR`): Maximal Marginal Relevance (MMR) reorders results to reduce redundancy. It trades off relevance vs diversity using <code>MMR_LAMBDA</code>.
- **Enable smart gating (Recall)** (`chat.recall_gate.enabled`): Recall is powerful, but querying it on every single message can add noise and slow chat down. Smart gating makes Recall <span class="tt-strong">message-aware</span>: it looks at cheap signals (patt...
- **Entity Types** (`ENTITY_TYPES`): Types of code entities to extract and store in the graph: function, class, module, variable, import. Each entity becomes a node in Neo4j with relationships to other entities it references or contains.
- **Fallback Confidence** (`FALLBACK_CONFIDENCE`): Confidence threshold that decides when to escalate to fallback retrieval strategies (e.g., rewrite the query, broaden candidate pools, or lean on alternative sources) instead of trusting the initia...
- **Graph Weight** (`GRAPH_WEIGHT`): Weight assigned to graph traversal results in tri-brid fusion. Higher values prioritize code relationships like call graphs and imports. Range: 0.0-1.0. Default: 0.3. Helps find related code across...
- **Light for short questions** (`chat.recall_gate.light_for_short_questions`): Enables a cheap Recall check for short questions that look like follow-ups (for example: “what about caching?” right after a discussion).<br><br>Light mode is intentionally minimal: it uses sparse-...
- **Light top_k** (`chat.recall_gate.light_top_k`): How many Recall snippets to include when intensity is <span class="tt-strong">light</span>.<br><br>Light is meant to be a quick sanity check. Keeping this small avoids pulling too much history into...
- **Max chunks per file** (`MAX_CHUNKS_PER_FILE`): Limits how many chunks from the same file can appear in final results. This improves document diversity and reduces “one big file dominates” behavior.
- **Max Hops** (`MAX_HOPS`): Maximum number of relationship traversals in graph search. Higher values explore more connected code but increase latency. Range: 1-5. Default: 2. Set to 1 for direct relationships only.
- **Max short-statement tokens** (`chat.recall_gate.skip_max_tokens`): A small threshold used by the gate to decide when a very short <span class="tt-strong">statement</span> should trigger a light Recall check.<br><br>Messages at or under this length (approximate wor...
- **Min score (graph)** (`MIN_SCORE_GRAPH`): Minimum score threshold for graph-leg candidates before fusion. Use this to drop very weak graph hits.
- **Min score (sparse)** (`MIN_SCORE_SPARSE`): Minimum score threshold for sparse-leg candidates before fusion. Use this to reduce noisy BM25/FTS matches.
- **Min score (vector)** (`MIN_SCORE_VECTOR`): Minimum score threshold for vector-leg candidates before fusion (in addition to vector similarity threshold). Use this to drop weak embedding matches.
- **MMR lambda (λ)** (`MMR_LAMBDA`): MMR relevance vs diversity tradeoff. <b>1.0</b> is pure relevance (no diversity), <b>0.0</b> is maximum diversity. Typical: 0.6–0.8.
- **Neighbor window** (`NEIGHBOR_WINDOW`): After selecting top hits, fetch adjacent chunks by <code>chunk_ordinal</code> within this window for better coherence (e.g., 1 pulls the previous and next chunk). Requires <code>EMIT_CHUNK_ORDINAL<...
- **Reciprocal Rank Fusion (K)** (`RRF_K_DIV`): Fusion parameter for combining BM25 + vector rankings: score += 1/(K+rank). Lower K increases influence of lower ranks; higher K flattens. Typical: 30–100 (60 recommended).
- **Relationship Types** (`RELATIONSHIP_TYPES`): Types of relationships to extract between code entities: calls (function invocations), imports (module dependencies), inherits (class hierarchy), contains (nesting), references (variable usage). De...
- **Resolved Tokenizer** (`BM25_TOKENIZER_RESOLVED`): The actual tokenization settings that will be applied during indexing. Shows the combined effect of tokenizer type, stemmer language, and stopwords language. This is the effective configuration aft...
- **Skip Dense Embeddings** (`SKIP_DENSE`): Skip vector embeddings and Qdrant during indexing to create a fast BM25-only (keyword-only) index. Useful for quick testing, CI/CD pipelines, or when Qdrant is unavailable. BM25-only mode is faster...
- **Skip greetings/acknowledgments** (`chat.recall_gate.skip_greetings`): Skips Recall for “pure conversation glue” like “hi”, “thanks”, “ok”, “got it”, and quick farewells. These almost never need memory, and pulling Recall for them tends to add irrelevant snippets.<br>...
- **Skip standalone questions** (`chat.recall_gate.skip_standalone_questions`): Skips Recall for questions that usually make sense without chat history (patterns like “what is…”, “how does…”, “explain…”, “where is…”). This keeps Recall from injecting unrelated older context in...
- **Skip when RAG active** (`chat.recall_gate.skip_when_rag_active`): When you already have one or more code corpora checked, you’re asking for repo-grounded answers. In that situation, Recall can sometimes crowd out code context or mix in old conversation threads.<b...
- **Sparse search engine** (`SPARSE_SEARCH_ENGINE`): Sparse retrieval engine.<br><br><b>postgres_fts</b>: built-in Postgres full text search (tsvector + tsquery).<br><b>pg_search_bm25</b>: ParadeDB pg_search BM25 index (requires extension). If unavai...
- **Sparse search highlight** (`SPARSE_SEARCH_HIGHLIGHT`): Experimental toggle for returning highlight snippets for sparse matches. Availability depends on the sparse engine and may be a no-op in some deployments.
- **Sparse search query mode** (`SPARSE_SEARCH_QUERY_MODE`): Controls how sparse queries are parsed.<br><br><b>plain</b>: simple keyword parsing.<br><b>phrase</b>: treat input as a phrase.<br><b>boolean</b>: allow boolean-style operators (implementation vari...
- **Sparse Weight** (`SPARSE_WEIGHT`): Weight assigned to sparse BM25 search results in tri-brid fusion. Higher values prioritize keyword matches. Range: 0.0-1.0. Default: 0.3. Effective for exact identifier matching in code.
- **Standard recency weight** (`chat.recall_gate.standard_recency_weight`): Balances “most similar” vs “most recent” when ranking Recall results.<br><br>At <span class="mono">0.0</span>, Recall ranking is purely similarity-based. As you raise it, newer messages get a stron...
- **Standard top_k** (`chat.recall_gate.standard_top_k`): How many Recall snippets to include in the normal <span class="tt-strong">standard</span> mode.<br><br>Higher values give the model more history to work with, but it also increases the chance of dr...
- **Top‑K Dense** (`TOPK_DENSE`): Number of candidate results to retrieve from Qdrant vector (semantic) search before hybrid fusion. Higher values (100-150) improve recall for semantic matches but increase query latency and memory ...
- **Top‑K Sparse** (`TOPK_SPARSE`): Number of candidate results to retrieve from BM25 keyword (lexical) search before hybrid fusion. Higher values (100-150) improve recall for exact keyword matches (variable names, function names, er...
- **Tri-Brid Fusion** (`TRIBRID_FUSION`): The core search methodology combining three retrieval approaches: vector (dense), sparse (BM25), and graph traversal. Results from each method are fused using RRF or weighted scoring to produce fin...
- **Vector Backend** (`VECTOR_BACKEND`): Selects the vector search backend used for dense retrieval. Postgres (pgvector) is the default backend in TriBridRAG and stores your embedding vectors for fast similarity search. Use this to switch...
- **Vector Weight (Hybrid Fusion)** (`VECTOR_WEIGHT`): Weight assigned to dense vector (semantic embedding) scores during hybrid search fusion. Dense embeddings capture semantic meaning and conceptual similarity, excelling at natural language queries a...

### ui

- **Editor Bind Address** (`EDITOR_BIND`): Network interface for editor service to bind to. Use 127.0.0.1 for localhost-only access (secure), 0.0.0.0 for network access (enable remote editing).
- **Editor Enabled** (`EDITOR_ENABLED`): Enable embedded code editor integration in GUI (1=yes, 0=no). Allows viewing and editing code snippets from retrieval results directly in browser.
- **Grafana Dashboard UID** (`GRAFANA_DASHBOARD_UID`): Unique identifier for default Grafana dashboard to display in GUI. Find UID in dashboard settings or URL (e.g., /d/abc123/dashboard-name -> UID is abc123).
- **GUI Theme** (`THEME_MODE`): Color theme for web GUI. Options: "light" (light mode), "dark" (dark mode), "auto" (follows system preference). Changes appearance immediately when toggled.
- **Layer Bonus (GUI)** (`LAYER_BONUS_GUI`): Score boost applied to chunks from GUI/frontend layers when query intent is classified as UI-related. Part of the multi-layer architecture routing system. When users ask "how does the settings page...
- **Show decision in status bar** (`chat.recall_gate.show_gate_decision`): Shows the selected Recall intensity and a short human-readable reason in the chat status bar after each message (for example: “Greeting — skipping Recall”).<br><br>Useful when you’re tuning Recall ...
- **Show raw signals (dev)** (`chat.recall_gate.show_signals`): Adds a “signals” expander in the chat debug footer that dumps the full RecallPlan JSON (signals, decision, and overrides).<br><br>This is meant for tuning and debugging. It’s intentionally noisy an...
- **UI Public Directory** (`GUI_DIR`): Directory for shared UI assets (for example: models.json) used by /api/models and the frontend. Defaults to ./web/public. Point this to a writable volume if you keep catalogs in sync at runtime; th...
